{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518204c-868f-4526-904d-405c8ea61d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"2\")\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"hola\")\n",
    "import numpy as np\n",
    "import tensorflow as tf  # For tf.data and preprocessing only.\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "!pip install deeplake\n",
    "import deeplake\n",
    "import cv2\n",
    "import json\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61ddcaf-7625-4821-aadc-518c50cf1dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 100\n",
    "input_shape = (256, 256, 3)\n",
    "\n",
    "patch_size = (16, 16)  # 2-by-2 sized patches\n",
    "dropout_rate = 0.03  # Dropout rate\n",
    "num_heads = 8  # Attention heads\n",
    "embed_dim = 128  # Embedding dimension\n",
    "num_mlp = 512  # MLP layer size\n",
    "# Convert embedded patches to query, key, and values with a learnable additive\n",
    "# value\n",
    "qkv_bias = True\n",
    "window_size = 8  # Size of attention window\n",
    "shift_size = 4  # Size of shifting window\n",
    "image_dimension = 256  # Initial image size\n",
    "\n",
    "num_patch_x = input_shape[0] // patch_size[0]\n",
    "num_patch_y = input_shape[1] // patch_size[1]\n",
    "\n",
    "learning_rate = 5e-4\n",
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "validation_split = 0.1\n",
    "weight_decay = 0.0001\n",
    "label_smoothing = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9ee212-0fc4-4dcd-8350-79c99e388b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset desde DeepLake\n",
    "ds = deeplake.query(\"\"\"SELECT * FROM \"hub://activeloop/plantvillage-with-augmentation\" WHERE labels IN ('Apple_healthy', 'Apple_scab')\"\"\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6f007-8314-4265-b901-0ac828bc8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer imágenes y etiquetas en formato numpy\n",
    "x_data = np.array([cv2.resize(img, (256, 256)) for img in ds['images']])\n",
    "y_data = np.array(ds['labels'][:])\n",
    "y_data = np.where(y_data == 12, 0, 1)\n",
    "\n",
    "#MAYBE DELETE\n",
    "indices = np.random.permutation(len(x_data))\n",
    "\n",
    "# Aplicar la permutación a los datos\n",
    "x_data = x_data[indices]\n",
    "y_data = y_data[indices]\n",
    "\n",
    "# Verificar la forma del dataset\n",
    "print(f\"Original shape - x_data: {x_data.shape}, y_data: {y_data.shape}\")\n",
    "\n",
    "# Normalizar valores de píxeles\n",
    "x_data = x_data / 255.0\n",
    "\n",
    "# Convertir etiquetas a one-hot encoding\n",
    "num_classes = len(np.unique(y_data))\n",
    "y_data = keras.utils.to_categorical(y_data, num_classes)\n",
    "\n",
    "# Dividir en train, validation y test\n",
    "validation_split = 0.1\n",
    "test_split = 0.1\n",
    "num_total_samples = len(x_data)\n",
    "num_train_samples = int(num_total_samples * (1 - validation_split - test_split))\n",
    "num_val_samples = int(num_total_samples * validation_split)\n",
    "\n",
    "x_train, x_val, x_test = np.split(x_data, [num_train_samples, num_train_samples + num_val_samples])\n",
    "y_train, y_val, y_test = np.split(y_data, [num_train_samples, num_train_samples + num_val_samples])\n",
    "\n",
    "# Imprimir las formas de los datasets\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_val shape: {x_val.shape} - y_val shape: {y_val.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Visualizar algunas imágenes del dataset\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0acde83-673d-4ba9-aa00-a4a490113e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size):\n",
    "    _, height, width, channels = x.shape\n",
    "    patch_num_y = height // window_size\n",
    "    patch_num_x = width // window_size\n",
    "    x = ops.reshape(\n",
    "        x,\n",
    "        (\n",
    "            -1,\n",
    "            patch_num_y,\n",
    "            window_size,\n",
    "            patch_num_x,\n",
    "            window_size,\n",
    "            channels,\n",
    "        ),\n",
    "    )\n",
    "    x = ops.transpose(x, (0, 1, 3, 2, 4, 5))\n",
    "    windows = ops.reshape(x, (-1, window_size, window_size, channels))\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, height, width, channels):\n",
    "    patch_num_y = height // window_size\n",
    "    patch_num_x = width // window_size\n",
    "    x = ops.reshape(\n",
    "        windows,\n",
    "        (\n",
    "            -1,\n",
    "            patch_num_y,\n",
    "            patch_num_x,\n",
    "            window_size,\n",
    "            window_size,\n",
    "            channels,\n",
    "        ),\n",
    "    )\n",
    "    x = ops.transpose(x, (0, 1, 3, 2, 4, 5))\n",
    "    x = ops.reshape(x, (-1, height, width, channels))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348b3e99-df6b-4dc4-8f1b-310b1f7ca3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        window_size,\n",
    "        num_heads,\n",
    "        qkv_bias=True,\n",
    "        dropout_rate=0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.proj = layers.Dense(dim)\n",
    "\n",
    "        num_window_elements = (2 * self.window_size[0] - 1) * (\n",
    "            2 * self.window_size[1] - 1\n",
    "        )\n",
    "        self.relative_position_bias_table = self.add_weight(\n",
    "            shape=(num_window_elements, self.num_heads),\n",
    "            initializer=keras.initializers.Zeros(),\n",
    "            trainable=True,\n",
    "        )\n",
    "        coords_h = np.arange(self.window_size[0])\n",
    "        coords_w = np.arange(self.window_size[1])\n",
    "        coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n",
    "        coords = np.stack(coords_matrix)\n",
    "        coords_flatten = coords.reshape(2, -1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.transpose([1, 2, 0])\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "\n",
    "        self.relative_position_index = keras.Variable(\n",
    "            initializer=relative_position_index,\n",
    "            shape=relative_position_index.shape,\n",
    "            dtype=\"int\",\n",
    "            trainable=False,\n",
    "        )\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        _, size, channels = x.shape\n",
    "        head_dim = channels // self.num_heads\n",
    "        x_qkv = self.qkv(x)\n",
    "        x_qkv = ops.reshape(x_qkv, (-1, size, 3, self.num_heads, head_dim))\n",
    "        x_qkv = ops.transpose(x_qkv, (2, 0, 3, 1, 4))\n",
    "        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n",
    "        q = q * self.scale\n",
    "        k = ops.transpose(k, (0, 1, 3, 2))\n",
    "        attn = q @ k\n",
    "\n",
    "        num_window_elements = self.window_size[0] * self.window_size[1]\n",
    "        relative_position_index_flat = ops.reshape(self.relative_position_index, (-1,))\n",
    "        relative_position_bias = ops.take(\n",
    "            self.relative_position_bias_table,\n",
    "            relative_position_index_flat,\n",
    "            axis=0,\n",
    "        )\n",
    "        relative_position_bias = ops.reshape(\n",
    "            relative_position_bias,\n",
    "            (num_window_elements, num_window_elements, -1),\n",
    "        )\n",
    "        relative_position_bias = ops.transpose(relative_position_bias, (2, 0, 1))\n",
    "        attn = attn + ops.expand_dims(relative_position_bias, axis=0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            mask_float = ops.cast(\n",
    "                ops.expand_dims(ops.expand_dims(mask, axis=1), axis=0),\n",
    "                \"float32\",\n",
    "            )\n",
    "            attn = ops.reshape(attn, (-1, nW, self.num_heads, size, size)) + mask_float\n",
    "            attn = ops.reshape(attn, (-1, self.num_heads, size, size))\n",
    "            attn = keras.activations.softmax(attn, axis=-1)\n",
    "        else:\n",
    "            attn = keras.activations.softmax(attn, axis=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        x_qkv = attn @ v\n",
    "        x_qkv = ops.transpose(x_qkv, (0, 2, 1, 3))\n",
    "        x_qkv = ops.reshape(x_qkv, (-1, size, channels))\n",
    "        x_qkv = self.proj(x_qkv)\n",
    "        x_qkv = self.dropout(x_qkv)\n",
    "        return x_qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8593ca-4fa4-4b6b-a976-44771fb29efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_patch,\n",
    "        num_heads,\n",
    "        window_size=7,\n",
    "        shift_size=0,\n",
    "        num_mlp=1024,\n",
    "        qkv_bias=True,\n",
    "        dropout_rate=0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dim = dim  # number of input dimensions\n",
    "        self.num_patch = num_patch  # number of embedded patches\n",
    "        self.num_heads = num_heads  # number of attention heads\n",
    "        self.window_size = window_size  # size of window\n",
    "        self.shift_size = shift_size  # size of window shift\n",
    "        self.num_mlp = num_mlp  # number of MLP nodes\n",
    "\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=(self.window_size, self.window_size),\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "        self.drop_path = layers.Dropout(dropout_rate)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "        self.mlp = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(num_mlp),\n",
    "                layers.Activation(keras.activations.gelu),\n",
    "                layers.Dropout(dropout_rate),\n",
    "                layers.Dense(dim),\n",
    "                layers.Dropout(dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if min(self.num_patch) < self.window_size:\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.num_patch)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.shift_size == 0:\n",
    "            self.attn_mask = None\n",
    "        else:\n",
    "            height, width = self.num_patch\n",
    "            h_slices = (\n",
    "                slice(0, -self.window_size),\n",
    "                slice(-self.window_size, -self.shift_size),\n",
    "                slice(-self.shift_size, None),\n",
    "            )\n",
    "            w_slices = (\n",
    "                slice(0, -self.window_size),\n",
    "                slice(-self.window_size, -self.shift_size),\n",
    "                slice(-self.shift_size, None),\n",
    "            )\n",
    "            mask_array = np.zeros((1, height, width, 1))\n",
    "            count = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    mask_array[:, h, w, :] = count\n",
    "                    count += 1\n",
    "            mask_array = ops.convert_to_tensor(mask_array)\n",
    "\n",
    "            # mask array to windows\n",
    "            mask_windows = window_partition(mask_array, self.window_size)\n",
    "            mask_windows = ops.reshape(\n",
    "                mask_windows, [-1, self.window_size * self.window_size]\n",
    "            )\n",
    "            attn_mask = ops.expand_dims(mask_windows, axis=1) - ops.expand_dims(\n",
    "                mask_windows, axis=2\n",
    "            )\n",
    "            attn_mask = ops.where(attn_mask != 0, -100.0, attn_mask)\n",
    "            attn_mask = ops.where(attn_mask == 0, 0.0, attn_mask)\n",
    "            self.attn_mask = keras.Variable(\n",
    "                initializer=attn_mask,\n",
    "                shape=attn_mask.shape,\n",
    "                dtype=attn_mask.dtype,\n",
    "                trainable=False,\n",
    "            )\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        height, width = self.num_patch\n",
    "        _, num_patches_before, channels = x.shape\n",
    "        x_skip = x\n",
    "        x = self.norm1(x)\n",
    "        x = ops.reshape(x, (-1, height, width, channels))\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = ops.roll(\n",
    "                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]\n",
    "            )\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        x_windows = window_partition(shifted_x, self.window_size)\n",
    "        x_windows = ops.reshape(\n",
    "            x_windows, (-1, self.window_size * self.window_size, channels)\n",
    "        )\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n",
    "\n",
    "        attn_windows = ops.reshape(\n",
    "            attn_windows,\n",
    "            (-1, self.window_size, self.window_size, channels),\n",
    "        )\n",
    "        shifted_x = window_reverse(\n",
    "            attn_windows, self.window_size, height, width, channels\n",
    "        )\n",
    "        if self.shift_size > 0:\n",
    "            x = ops.roll(\n",
    "                shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]\n",
    "            )\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        x = ops.reshape(x, (-1, height * width, channels))\n",
    "        x = self.drop_path(x, training=training)\n",
    "        x = x_skip + x\n",
    "        x_skip = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.drop_path(x)\n",
    "        x = x_skip + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f94ad3-04db-41b7-9987-6c4641556c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using tf ops since it is only used in tf.data.\n",
    "def patch_extract(images):\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    patches = tf.image.extract_patches(\n",
    "        images=images,\n",
    "        sizes=(1, patch_size[0], patch_size[1], 1),\n",
    "        strides=(1, patch_size[0], patch_size[1], 1),\n",
    "        rates=(1, 1, 1, 1),\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "    patch_dim = patches.shape[-1]\n",
    "    patch_num = patches.shape[1]\n",
    "    return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n",
    "\n",
    "\n",
    "class PatchEmbedding(layers.Layer):\n",
    "    def __init__(self, num_patch, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patch = num_patch\n",
    "        self.proj = layers.Dense(embed_dim)\n",
    "        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, patch):\n",
    "        pos = ops.arange(start=0, stop=self.num_patch)\n",
    "        return self.proj(patch) + self.pos_embed(pos)\n",
    "\n",
    "\n",
    "class PatchMerging(keras.layers.Layer):\n",
    "    def __init__(self, num_patch, embed_dim):\n",
    "        super().__init__()\n",
    "        self.num_patch = num_patch\n",
    "        self.embed_dim = embed_dim\n",
    "        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        height, width = self.num_patch\n",
    "        _, _, C = x.shape\n",
    "        x = ops.reshape(x, (-1, height, width, C))\n",
    "        x0 = x[:, 0::2, 0::2, :]\n",
    "        x1 = x[:, 1::2, 0::2, :]\n",
    "        x2 = x[:, 0::2, 1::2, :]\n",
    "        x3 = x[:, 1::2, 1::2, :]\n",
    "        x = ops.concatenate((x0, x1, x2, x3), axis=-1)\n",
    "        x = ops.reshape(x, (-1, (height // 2) * (width // 2), 4 * C))\n",
    "        return self.linear_trans(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c749316-2cd6-4654-a981-1928ce3caf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def augment(x):\n",
    "    x = tf.image.random_crop(x, size=(image_dimension, image_dimension, 3))\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5157f01e-d219-49d2-8512-f4086a698c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .map(lambda x, y: (augment(x), y))\n",
    "    .batch(batch_size=batch_size)\n",
    "    .map(lambda x, y: (patch_extract(x), y))\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf0cea0-13a7-41e4-8936-c9deafe13684",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    .batch(batch_size=batch_size)\n",
    "    .map(lambda x, y: (patch_extract(x), y))\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06639157-244a-4e7c-a760-d29f5449bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    .batch(batch_size=batch_size)\n",
    "    .map(lambda x, y: (patch_extract(x), y))\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3576e9db-7313-4120-b228-4e7c14cb3208",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = layers.Input(shape=(256, 768))\n",
    "x = PatchEmbedding(num_patch_x * num_patch_y, embed_dim)(input)\n",
    "x = SwinTransformer(\n",
    "    dim=embed_dim,\n",
    "    num_patch=(num_patch_x, num_patch_y),\n",
    "    num_heads=num_heads,\n",
    "    window_size=window_size,\n",
    "    shift_size=0,\n",
    "    num_mlp=num_mlp,\n",
    "    qkv_bias=qkv_bias,\n",
    "    dropout_rate=dropout_rate,\n",
    ")(x)\n",
    "x = SwinTransformer(\n",
    "    dim=embed_dim,\n",
    "    num_patch=(num_patch_x, num_patch_y),\n",
    "    num_heads=num_heads,\n",
    "    window_size=window_size,\n",
    "    shift_size=shift_size,\n",
    "    num_mlp=num_mlp,\n",
    "    qkv_bias=qkv_bias,\n",
    "    dropout_rate=dropout_rate,\n",
    ")(x)\n",
    "x = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim)(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "output = layers.Dense(num_classes, activation=\"softmax\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e604d809-3880-436a-8197-0d2c684831e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(input, output)\n",
    "model.compile(\n",
    "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n",
    "    optimizer=keras.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    ),\n",
    "    metrics=[\n",
    "        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "        keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        keras.metrics.Precision(name=\"precision\"),\n",
    "        keras.metrics.Recall(name=\"recall\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=dataset_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff5bf46-3e43-4810-8a5d-6b0115452fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d08bb48-276a-403b-8a9c-4d17b12c77ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, top_5_accuracy, precision, recall = model.evaluate(dataset_test)\n",
    "print(f\"Test loss: {round(loss, 2)}\")\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "print(f\"Test precision: {round(precision * 100, 2)}%\")\n",
    "print(f\"Test recall: {round(recall * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e12cc43-1c9c-4394-8576-3a4bf903bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred_probs = model.predict(dataset_test)  \n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)  # Convertir a clases (0 o 1)\n",
    "\n",
    "# Obtener etiquetas verdaderas\n",
    "y_true = np.concatenate([y for _, y in dataset_test], axis=0)  # Extraer etiquetas\n",
    "y_true = np.argmax(y_true, axis=1)  # Convertir a clases si están en one-hot\n",
    "\n",
    "# Crear la matriz de confusión\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Etiquetas personalizadas\n",
    "labels = [\"Healthy\", \"Unhealthy\"]\n",
    "\n",
    "# Visualizar la matriz de confusión\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
